{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1290f794-9477-4e45-ab20-81d04c040fc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## langcain agent demo for ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf80354-7edd-4403-9c8b-97a4712f4dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install langchain[all]\n",
    "#!pip install sagemaker --upgrade\n",
    "#!pip install  boto3\n",
    "#!pip install requests_aws4auth\n",
    "#!pip install opensearch-py\n",
    "#!pip install pydantic==1.10.0\n",
    "#!pip install PyAthena[SQLAlchemy]==1.0.0\n",
    "#!pip install PyAthena[JDBC]==1.0.0\n",
    "#!pip install openai\n",
    "!pip install sqlalchemy-redshift\n",
    "!pip install redshift_connector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b216f11-a2b5-4f0b-b4de-060a1a64076a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## initial sagemaker env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19f05414-d44b-464e-a0c7-46c317378ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::687912291502:role/service-role/AmazonSageMaker-ExecutionRole-20211013T113123\n",
      "sagemaker bucket: sagemaker-us-west-2-687912291502\n",
      "sagemaker session region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "from typing import Dict\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3aed78-e14b-4d30-93d3-2871d6603bd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## intial lanchain lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "4aae9f94-a8df-49b5-b22b-f89e8d756b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import ConversationBufferWindowMemory,ConversationBufferMemory\n",
    "from langchain import LLMChain\n",
    "from typing import Dict\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"]= \"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\"\n",
    "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "aos_endpoint=\"vpc-llm-rag-aos-seg3mzhpp76ncpxezdqtcsoiga.us-west-2.es.amazonaws.com\"\n",
    "region='us-west-2'\n",
    "username=\"admin\"\n",
    "passwd=\"(OL>0p;/\"\n",
    "index_name=\"qa_index\"\n",
    "size=10\n",
    "\n",
    "## for chatglm\n",
    "class TextGenContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        #input_str = json.dumps({prompt: prompt, **model_kwargs})\n",
    "        input_str = json.dumps({\n",
    "                \"ask\": prompt,\n",
    "                \"parameters\": model_kwargs\n",
    "            })\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"answer\"]\n",
    "\n",
    "### for vicuna/llama\n",
    "class TextGenContentHandler2(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\n",
    "                \"input\": prompt,\n",
    "                \"parameters\": model_kwargs\n",
    "            })\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"data\"][0][\"generated_text\"]   \n",
    "\n",
    "## for embedding\n",
    "class ContentHandler(EmbeddingsContentHandler):\n",
    "    parameters = {\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"temperature\": 0,\n",
    "        \"min_length\": 10,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "    }\n",
    "    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": inputs, **model_kwargs})\n",
    "        return input_str.encode('utf-8')\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"sentence_embeddings\"]\n",
    "\n",
    "\n",
    "    \n",
    "sm_embeddings = SagemakerEndpointEmbeddings(\n",
    "    # endpoint_name=\"endpoint-name\", \n",
    "    # credentials_profile_name=\"credentials-profile-name\", \n",
    "    #endpoint_name=\"huggingface-textembedding-bloom-7b1-fp1-2023-04-17-03-31-12-148\", \n",
    "    endpoint_name=\"st-paraphrase-mpnet-base-v2-2023-04-17-10-05-10-718-endpoint\",\n",
    "    region_name=\"us-west-2\", \n",
    "    content_handler=embedding_content_handler\n",
    ")\n",
    "\n",
    "\n",
    "parameters = {\n",
    "  \"early_stopping\": False,\n",
    "  #\"length_penalty\": 2.0,\n",
    "  #\"max_new_tokens\": 500,\n",
    "  \"temperature\": 0.6,\n",
    "  \"max_length\": 8000,\n",
    "  #\"no_repeat_ngram_size\": 2,\n",
    "}\n",
    "sm_llm=SagemakerEndpoint(\n",
    "        #endpoint_name=\"chatglm-inference-0524-2023-06-01-07-11-27-379\",\n",
    "        endpoint_name=\"vicuna-7B-2023-06-04-13-07-39-746-endpoint\",\n",
    "        region_name=\"us-west-2\", \n",
    "        model_kwargs=parameters,\n",
    "        content_handler=text_gen_content_handler3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e9330b-0204-4e5a-bc7b-36ec7fd20818",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## func for agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0819cc2-5be1-4f78-8826-90c3ff2c72d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import os\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain import LLMChain\n",
    "\n",
    "\n",
    "\n",
    "def aos_knn_search(client, q_embedding, index, size=10):\n",
    "    query = {\n",
    "        \"size\": size,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"embedding\": {\n",
    "                    \"vector\": q_embedding,\n",
    "                    \"k\": size\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    opensearch_knn_respose = []\n",
    "    query_response = client.search(\n",
    "        body=query,\n",
    "        index=index\n",
    "    )\n",
    "    opensearch_knn_respose = [{'idx':item['_source'].get('idx',1),'doc_category':item['_source']['doc_category'],'doc_title':item['_source']['doc_title'],'id':item['_id'],'doc':\"{}{}{}\".format(item['_source']['doc'], QA_SEP, item['_source']['content']),\"doc_type\":item[\"_source\"][\"doc_type\"],\"score\":item[\"_score\"]}  for item in query_response[\"hits\"][\"hits\"]]\n",
    "    return opensearch_knn_respose\n",
    "\n",
    "def aos_reverse_search(client, index_name, field, query_term, exactly_match=False, size=10):\n",
    "    \"\"\"\n",
    "    search opensearch with query.\n",
    "    :param host: AOS endpoint\n",
    "    :param index_name: Target Index Name\n",
    "    :param field: search field\n",
    "    :param query_term: query term\n",
    "    :return: aos response json\n",
    "    \"\"\"\n",
    "    if not isinstance(client, OpenSearch):   \n",
    "        client = OpenSearch(\n",
    "            hosts=[{'host': client, 'port': 443}],\n",
    "            http_auth = awsauth,\n",
    "            use_ssl=True,\n",
    "            verify_certs=True,\n",
    "            connection_class=RequestsHttpConnection\n",
    "        )\n",
    "    query = None\n",
    "    if exactly_match:\n",
    "        query =  {\n",
    "            \"query\" : {\n",
    "                \"match_phrase\":{\n",
    "                    \"doc\": {\n",
    "                        \"query\": query_term,\n",
    "                        \"analyzer\": \"ik_smart\"\n",
    "                      }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        query = {\n",
    "            \"size\": size,\n",
    "            \"query\": {\n",
    "                \"bool\": {\n",
    "                    \"should\": [{\n",
    "                            \"bool\": {\n",
    "                                \"must\": [{\n",
    "                                        \"term\": {\n",
    "                                            \"doc_type\": \"Question\"\n",
    "                                        }\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"match\": {\n",
    "                                            \"doc\": query_term\n",
    "                                        }\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\n",
    "                        },\n",
    "                        {\n",
    "                            \"bool\": {\n",
    "                                \"must\": [{\n",
    "                                        \"term\": {\n",
    "                                            \"doc_type\": \"Paragraph\"\n",
    "                                        }\n",
    "                                    },\n",
    "                                    {\n",
    "                                        \"match\": {\n",
    "                                            \"content\": query_term\n",
    "                                        }\n",
    "                                    }\n",
    "                                ]\n",
    "                            }\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            },\n",
    "            \"sort\": [{\n",
    "                \"_score\": {\n",
    "                    \"order\": \"desc\"\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "    query_response = client.search(\n",
    "        body=query,\n",
    "        index=index_name\n",
    "    )\n",
    "\n",
    "    if exactly_match:\n",
    "        result_arr = [ {'idx':item['_source'].get('idx',0),'doc_category':item['_source']['doc_category'],'doc_title':item['_source']['doc_title'],'id':item['_id'],'doc': item['_source']['content'], 'doc_type': item['_source']['doc_type'], 'score': item['_score']} for item in query_response[\"hits\"][\"hits\"]]\n",
    "    else:\n",
    "        result_arr = [ {'idx':item['_source'].get('idx',0),'doc_category':item['_source']['doc_category'],'doc_title':item['_source']['doc_title'],'id':item['_id'],'doc':\"{}{}{}\".format(item['_source']['doc'], QA_SEP, item['_source']['content']), 'doc_type': item['_source']['doc_type'], 'score': item['_score']} for item in query_response[\"hits\"][\"hits\"]]\n",
    "\n",
    "    return result_arr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_vector_by_sm_endpoint(questions, sm_client, endpoint_name):\n",
    "    parameters = {\n",
    "    }\n",
    "\n",
    "    response_model = sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(\n",
    "            {\n",
    "                \"inputs\": questions,\n",
    "                \"parameters\": parameters,\n",
    "                \"is_query\" : True,\n",
    "                \"instruction\" :  instruction_en\n",
    "            }\n",
    "        ),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "    json_str = response_model['Body'].read().decode('utf8')\n",
    "    json_obj = json.loads(json_str)\n",
    "    embeddings = json_obj['sentence_embeddings']\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_topk_items(opensearch_query_response, topk=5):\n",
    "    opensearch_knn_nodup = []\n",
    "    unique_ids = set()\n",
    "    for item in opensearch_query_response:\n",
    "        if item['id'] not in unique_ids:\n",
    "            opensearch_knn_nodup.append((item['doc'], item['score'], item['idx'], item['doc_title'],item['id'],item['doc_category'],item['doc_type']))\n",
    "            unique_ids.add(item['id'])\n",
    "    return opensearch_knn_nodup\n",
    "\n",
    "\n",
    "\n",
    "def k_nn_ingestion_by_aos(docs,index,hostname,username,passwd):\n",
    "    auth = (username, passwd)\n",
    "    search = OpenSearch(\n",
    "        hosts = [{'host': hostname, 'port': 443}],\n",
    "        ##http_auth = awsauth ,\n",
    "        http_auth = auth ,\n",
    "        use_ssl = True,\n",
    "        verify_certs = True,\n",
    "        connection_class = RequestsHttpConnection\n",
    "    )\n",
    "    for doc in docs:\n",
    "        vector_field = doc['sentence_vector']\n",
    "        question_filed = doc['question']\n",
    "        answer_field = doc['answer']\n",
    "        document = { \"question\": question_filed, 'answer':answer_field, \"sentence_vector\": vector_field}\n",
    "        search.index(index=index, body=document)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa302e0a-b8ed-4fa0-934e-ab4ff0740316",
   "metadata": {
    "tags": []
   },
   "source": [
    "## major chain pipeline ################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ca53df-0f55-46ec-85a9-36316e25008b",
   "metadata": {},
   "source": [
    "### 0: index 创建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ccff8-4361-4f77-a42f-9d2c64ebe387",
   "metadata": {},
   "source": [
    "PUT metadata-index\n",
    "{\n",
    "    \"settings\" : {\n",
    "        \"index\":{\n",
    "            \"number_of_shards\" : 5,\n",
    "            \"number_of_replicas\" : 0,\n",
    "            \"knn\": \"true\",\n",
    "            \"knn.algo_param.ef_search\": 32\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"metadata_type\" : {\n",
    "                \"type\" : \"keyword\"\n",
    "            },\n",
    "            \"database_name\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"ik_max_word\",\n",
    "                \"search_analyzer\": \"ik_smart\"\n",
    "            },\n",
    "            \"table_name\": {\n",
    "                \"type\": \"text\"\n",
    "            },\n",
    "            \"table_desc_embedding\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 768,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"l2\",\n",
    "                    \"engine\": \"faiss\",\n",
    "                    \"parameters\": {\n",
    "                        \"ef_construction\": 512,\n",
    "                        \"m\": 32\n",
    "                    }\n",
    "                }            \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ef73a-d691-4d30-b35e-1a5254cdb8d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1: data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bb9c564-9d44-4ba3-a279-34b6df2aa3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_question = \"\"\"在中国区是否可用？\n",
    "为什么在合成的小数据集上第一次查询的时候需要几分钟才返回？\n",
    "能支持多大规模数据的查询？查询速度怎么样？\n",
    "AWS Clean Rooms 是如何计费的？\n",
    "AWS Clean Rooms 从哪里可以看到CRPU-hours的用量？\n",
    "目前可以支持什么数据源的接入？ \n",
    "一个协作中，最大的并发查询数是多少？\n",
    "我们如何说服客户相信洁净室的安全性和正确性？我在这里的大多数合规计划中都看不到Clean Rooms， https://aws.amazon.com/compliance/services-in-scope/ ？\n",
    "数据源必须在AWS上么？\n",
    "数据是如何进入到S3？\n",
    "这些安全控制权限只是作用与分析么？能够改动它方的数据么？\n",
    "协作方的数据会移动么？协作方的原始数据会集中到Clean rooms 吗？\n",
    "是否发起者和数据贡献者都会被收费？\n",
    "数据贡献方的S3， 会产生API会产生调用次数收费么？\n",
    "是否有一个强约束，两方的数据中一定要一个join字段才能够进行分析？\n",
    "如果已经在用Athena，S3桶中已经有数据了，是否能基于S3中这个数据就地加入AWS Clean Room?\n",
    "是否能通过SDK也就是代码来调用Clean room的联合分析\n",
    "在输出分析结果的时候，能否按照字段进行分区？\n",
    "最大的参与方是多少？ 如果超过了限制怎么办？\n",
    "AWS clean rooms 用的什么加密算法？\n",
    "加密过程中有密文落地么？如果有，密文存在哪里？\n",
    "数据上传到S3的过程中，有两种加密方式Server Side 和 Client Side加密，如果客户的安全等级比较高，在数据上传之前做了加密，再想交给clean room 去处理，之前提到的C3R的加密方式，具体是一个怎么样的流程呢？\n",
    "AWS Clean Room在安全计算方面，应该归属哪一类？\n",
    "是否所有的字段都可以进行加密？\n",
    "如果要对某个字段进行求和，求平均的数据计算，是否可以加密？\n",
    "如果想要通过某些字段进行where过滤，这些字段应该是什么类型？\n",
    "C3R客户端是否有实现任何non-standard的加密算法?\n",
    "AWS Clean rooms 与 AMC的区别与联系是什么？\n",
    "AWS Clean rooms 与 其他的clean room服务商的区别？\n",
    "有哪些典型的应用场景？\n",
    "AWS Clean Rooms 的 data catalog 是如何实现的？ data sharing permission 是如何实现的？\n",
    "可以在哪些地方进行Clean room的联合分析？\n",
    "数据提供方如果对联合分析的收益方进行收费，或者实现一个数据授权的合同？\n",
    "AWS Clean Rooms 与 AWS Data Exchange 是什么关系？\n",
    "AWS Clean Rooms中是否支持视图？\n",
    "如果数据合作方没有aws account，能否支持？\n",
    "是否能够支持这个协作中，仅仅允许指定运行固定的SQL？\n",
    "AWS Clean Rooms可以让数据贡献者提供一些样例数据进行预览么？\n",
    "当一个数据贡献者的数据发生更新后会怎么样？\n",
    "AWS Clean Rooms 未来有哪些前进的方向？\n",
    "Service Team 有哪些相关的同事\n",
    "\"\"\"\n",
    "questions = all_question.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "565ebb18-ac48-4e27-8e86-a49cd221398f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_answer = \"\"\"目前没有落地中国区的时间表，已经在以下区域推出：美国东部（弗吉尼亚州北部）、美国东部（俄亥俄州）、美国西部（俄勒冈州）、亚太地区（首尔）、亚太地区（新加坡）、亚太地区（悉尼）、亚太地区（东京）、欧洲地区（法兰克福）、欧洲地区（爱尔兰）、欧洲地区（伦敦）和欧洲地区（斯德哥尔摩）\n",
    "第一次查询的时候是因为调度和拉起资源的影响，一般第二次查询就会变快。 但过一段时间后，资源释放后这个问题又会出现。在资源拉起期间是不进行收费的。\n",
    "能支持TB/GB级数据的查询。 一般查询延迟为几十秒到几分钟。默认计算容量为32 CRPUs, 目前这个默认计算容量不可设置，但是roadmap中未来打算让用户可以进行设置。(Slack中Ryan 提到，如果引擎中任务有积压，它能够scale up）\n",
    "按照CRPU-hour单价进行计费，每个查询默认计算容量为32 CRPUs。 金额 = (0.125 hours x 32 CRPUs * $0.656 per CRPU-hour) , 有1分钟的最小计费时间。头12个月内，会有9CRPU hours的免费额度。\n",
    "AWS Clean Rooms 本身的workspace中无法查看，可以在AWS Billing/Bills 中查看Usage Quantity得到该信息。\n",
    "目前只支持S3，其他数据源近期没有具体计划。\n",
    "5个\n",
    "正在加入这些合规计划的进程中。\n",
    "对，目前必须在AWS上，而且必须是同一个region。\n",
    "需要数据的持有者，把数据上传到S3上，然后再用Glue爬去下，拿到表的schema。这样才能关联到AWS CleanRooms。\n",
    "对，只作用于分析，不能修改对方数据\n",
    "在联合分析获取他方数据时数据存在移动，但协作方的原始数据并不会存住在clean room内，clean room并不是一个物理存贮空间。\n",
    "是单方收费，只有查询的接收方会进行收费。\n",
    "会， Glue Data Catalog API 的调用也会被收费， 如果加密数据用了KMS-CMK也会被相应的收费。\n",
    "List 和Aggregation两种不同的分析规则下有区别， List 只能支持重合用户的，所以必须要有关联字段。Aggregation可以支持你仅仅去查询对方的数据，这种情况下，是可以不指定关联字段的。\n",
    "是的，就地就能加入clean room, 这个S3桶就是一般的S3桶，并没有任何特殊。但这个S3路径不能注册到AWS Lake Formation中。\n",
    "可以，可以参考代码 https://gitlab.aws.dev/rmalecky/aws-clean-rooms-notebooks/-/blob/main/single_collaborator_aggregation.ipynb\n",
    "目前不能\n",
    "目前5个参与方为最大限制，这个是软性的限制，slack频道中有披露最大支持的硬限制为10.\n",
    "C3R，是aws开源加密代码库。提供了C3R Client(一个可执行的Jar包)，目前仅支持对csv和parquet文件格式进行加密，后续可能会支持更多格式。 由于clean room把所有的字段分成三种类型： 指纹列(fingerprint column), 密封列(sealed column), 明文列(cleartext column), 他们的加密方式有所不同，C3R client 会使用AES-GCM加密算法对sealed字段进行加密，会使用HMAC(Hash-based Message Authentication Code)来对fingerprint字段进行加密。\n",
    "由于C3R是客户端加密，所以clean room 关联S3中的数据已经是加密后的密文。\n",
    "首先Clean room 对于Server Side的加密是透明的，无需额外处理。Clean rooms 不支持S3的客户端加密，必须采用C3R客户端进行加密，加密完成以后把数据上传到S3桶，后续流程和不加密的流程是一致的。 加密这步需要比较多的手工操作，包括：* 加密前需要创建好collaboration(协作), 得到collaboration_id后续在加密中需要提供 * 利用openssl 生成32位密钥并分享给其他协作方。* 加密过程中需要指定哪些字段为指纹列(fingerprint column), 密封列(sealed column), 明文列(cleartext column)\n",
    "(待补充)\n",
    "对于sealed和 fingerprint字段，只有string字段类型被支持。对于csv文件，C3R的客户端处理任何值都作为UTF-8编码的文本，加密前不会做任何其他的前置处理。对于parquet文件，对sealed和 fingerprint字段，如果出现非string的字段，会直接报错。C3R 客户端不能处理parquet中的复杂字段比如struct。\n",
    "不能，只能作为cleartext明文列\n",
    "基本都是标准化的算法，除了一个HKDF(一种密钥推导函数)的实现(来自RFC5869), 但是使用的是java标准加密库中的MAC算法。\n",
    "AMC是一个专门服务与Amazon Ads的clean room应用，它是并且将持续是唯一的服务于Amazon Ads客户的应用服务。AWS Clean Rooms 是一个云分析服务，会服务于各个行业的数据合作需求。2023年, AMC 将会把自己的查询引擎和计算基础设置迁移到AWS Clean Rooms服务，将会帮助AMC更方便的服务于客户(他们将不再需要把自己第一方数据上传到AMC，在AWS S3上即可使用).\n",
    "AWS Clean rooms 覆盖的客户范围更广。而其他的服务商客户范围相对小，需要把数据移动到他们的平台上。AWS Clean rooms则无需数据移动。(Bastion 2021年announce的时候，其他的云厂商比如(google cloud 和 microsoft azure) 还没有通用场景的clean room solution，snowflake 有，功能上有区别，只允许data provider 提供预先固定的SQL，Bastion灵活性更好。snowflake要求数据必须进入他们的数仓，aws在S3 即可)。\n",
    "\"\"\"\n",
    "answers=all_answer.split(\"\\n\")\n",
    "\n",
    "answer_2 = \"\"\"\n",
    "包含多个行业，下面提供部分参考\n",
    "* 广告营销领域：\n",
    "    * 需要进行广告营销活动，流量平台方需要给广告主或者营销方他们的广告点击数据和展现数据进行分析，但是不能提供用户级别的信息。\n",
    "    * 典型客户：营销方或广告主 P&G, Barclays，媒体-流量平台 Amazon Ads, Comcast, NBC Universal\n",
    "* 零售领域\n",
    "    * 银行和零售商需要获取他们的重叠用户，用于进行联合的市场活动，但不要把客户的其他信息暴露给彼此。（比如非场合用户）\n",
    "* 医疗健康领域 (结合之前的HCLS Slides， 并不是来自Bastion)\n",
    "    * 药厂和医院之间，药厂需要医院的病历数据； 药厂和外包的研发机构之间需要进行数据的共享。\n",
    "    * 典型客户：Change Healthcare, AstraZeneca \n",
    "* 其他领域的一些典型的客户\n",
    "    * 数据服务商 Foursquare ，Nielsen， IRI， \n",
    "    * 其他 Cars.com\n",
    "\"\"\"\n",
    "answers.append(answer_2)\n",
    "    \n",
    "all_answer3=\"\"\"都是利用了AWS Lake formation, AWS Clean Rooms 里 SQL中字段级别的限制约束，是通过一种new class of AWS Lake formation permission 来实现的。\n",
    "可以在clean room 的 workspace， 也可以在Redshift workspace （Note: 从目前发布产品文档上并没有，但是说明背后的引擎就是redshift severless)\n",
    "需要通过AWS Data Exchange 来进行 （Note: 目前AWS Clean Rooms并没有体现）\n",
    "AWS Clean Rooms 可以通过AWS Data Exchange 去浏览和寻找可用数据的合作方。 他是AWS Data Exchange的更近一步的服务，提供了可控(多种约束限制)和可审计的数据合作方式\n",
    "允许客户在clean room 创建视图，并且在AWS Clean Rooms中保存物化视图，一旦退出协作，AWS Lake formation permission 将会被撤销，这些物化视图会被删除。（Note: 目前AWS Clean Rooms并没有体现）\n",
    "目前这个版本不支持，后续的版本可能会考虑（NBC Universal 希望对于没有aws账号另外一方的数据可用）\n",
    "可以，可以利用query template来做（Note: 目前AWS Clean Rooms并没有体现）\n",
    "可以这么做，可以提供一些没有任何约束的示例数据给用户（Note: 目前AWS Clean Rooms并没有体现）\n",
    "它是一种live的共享，任何更新会立刻反映到联合分析的结果中\n",
    "\"\"\"\n",
    "answers=answers+(all_answer3.split(\"\\n\"))\n",
    "\n",
    "answer_4=\"\"\"\n",
    "主要有四个方向：\n",
    "1. Identity matching 身份ID对齐 (Note: 目前这项在官方的PPT在有体现)\n",
    "2. 对隐私攻击的防护， 有些查询即使是一些聚合分析，仍然可能探查到个人的信息\n",
    "    1. 限制访问同一块范围数据的query的数量\n",
    "    2. 采用差分隐私(Differential Privacy) 结果中添加噪声(会影响分析的精度)，Morgen Stanley 作为这个功能的beta用户\n",
    "3. 机器学习，P&G 表现出这方面的需求， 应该也是基于表格数据的模型，可能是流失预测，人群聚类等场景\n",
    "4. 和DSP的集成，直接把激活用户ID给到对接的DSP，而不通过cleanroom的数据接收方\n",
    "\"\"\"\n",
    "answers.append(answer_4)\n",
    "               \n",
    "answer_5=\"\"\"Horne, Bill <bgh@amazon.com>, Rababy, Bethany <rababyb@amazon.com>, Malecky, Ryan <rmalecky@amazon.com>, Malik, Mohsen <mmohsen@amazon.com>, Tanna, Shamir <tannas@amazon.com>\"\"\"\n",
    "answers.append(answer_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1a8099c8-53e1-4581-9e87-381f5cf90020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"./code/\")\n",
    "#import func\n",
    "\n",
    "\n",
    "parameters = {\n",
    "      #\"early_stopping\": True,\n",
    "      #\"length_penalty\": 2.0,\n",
    "      \"max_new_tokens\": 50,\n",
    "      \"temperature\": 0,\n",
    "      \"min_length\": 10,\n",
    "      \"no_repeat_ngram_size\": 2,\n",
    "}\n",
    "endpoint_name=\"st-paraphrase-mpnet-base-v2-2023-04-17-23-30-38-088-endpoint\"\n",
    "##########embedding by llm model##############\n",
    "sentense_vectors = []\n",
    "for question in questions:\n",
    "    sentense_vectors=sentense_vectors + get_vector_by_sm_endpoint([question],sm_client,endpoint_name,parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a20d5-fdd0-4be2-835d-8afc753062a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs=[]\n",
    "for index, sentence_vector in enumerate(sentense_vectors):\n",
    "    #print(index, sentence_vector)\n",
    "    doc = {\n",
    "        \"question\":questions[index],\n",
    "        \"answer\": answers[index],\n",
    "        \"sentence_vector\": sentence_vector\n",
    "          }\n",
    "    docs.append(doc)\n",
    "#data = [{str(i): j for i, j in zip(['question', 'answer','sentence_vector'], values)} for values in zip(questions, answers,sentense_vectors)]\n",
    "#docs = json.dumps(data)\n",
    "#print((docs[0]['sentence_vector']))\n",
    "\n",
    "#########ingestion into aos ###################\n",
    "k_nn_ingestion_by_aos(docs,index_name,hostname,username,passwd)\n",
    "#k-nn_ingestion_by_lanchain(docs,opensearch_vector_search)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "634c6070-af8c-40b7-b655-e6eb37eff456",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT c_customer_id FROM web_sales ORDER BY ws_sold_price DESC LIMIT 1\n"
     ]
    }
   ],
   "source": [
    "sql_cmd=\"\"\"\n",
    "You are a MySQL expert. Given an input question, first create a syntactically correct MySQL query to run, then look at the results of the query and return the answer to the input question.\n",
    "Unless the user specifies in the question a specific number of examples to obtain, query for at most 3 results using the LIMIT clause as per MySQL. You can order the results to return the most informative data in the database.\n",
    "Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in backticks (`) to denote them as delimited identifiers.\n",
    "Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "Pay attention to use CURDATE() function to get the current date, if the question involves \"today\".\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: Question here\n",
    "SQLQuery: SQL Query to run\n",
    "SQLResult: Result of the SQLQuery\n",
    "Answer: Final answer here\n",
    "\n",
    "Only use the following tables:\n",
    "\n",
    "Question: 我需要知道销售报表中，下单金额最大的客户id\n",
    "SQLQuery: SELECT c_customer_id FROM web_sales ORDER BY ws_sold_price DESC LIMIT 1\n",
    "SQLResult\n",
    "SQL Query to run\"\"\"\n",
    "pattern = r\"SQLQuery: (.*?)\\nSQLResult\"\n",
    "matches = re.findall(pattern, sql_cmd)\n",
    "match = matches[1]\n",
    "sql_cmd = match\n",
    "sql_cmd=sql_cmd.replace(\"SQLQuery:\",\"\")\n",
    "sql_cmd=sql_cmd.replace(\"SQLResult\",\"\")\n",
    "sql_cmd=sql_cmd.replace(\"\\\\\",\"\")\n",
    "print(sql_cmd) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b98b6e5-4ee6-4749-8aa5-c7c8a48b95ea",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2:自定义Agent ，定制context\n",
    "1:自定义AOS倒排及knn检索tools    \n",
    "2:自定义中文Sql Agent 的ReAct prompt 前缀   \n",
    "3:使用customerSqlDatabaseChain+Sql Agent触发   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beac4e51-32b7-460f-adee-b20151163e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.utilities import BingSearchAPIWrapper\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")\n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "region = boto3.Session().region_name\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, 'es', session_token=credentials.token)\n",
    "\n",
    "class CustomEmbeddingSearchTool(BaseTool):\n",
    "    name = \"custom_knn_search\"\n",
    "    aos_client = OpenSearch(\n",
    "                hosts=[{'host': aos_endpoint, 'port': 443}],\n",
    "                http_auth = awsauth,\n",
    "                use_ssl=True,\n",
    "                verify_certs=True,\n",
    "                connection_class=RequestsHttpConnection)\n",
    "    aos_index=\"metadata_labels\"\n",
    "        \n",
    "    def _run(\n",
    "        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Opensearch 向量检索.\"\"\"\n",
    "        start = time.time()\n",
    "        query_embedding = get_vector_by_sm_endpoint(query, sm_client, sm_embeddings)\n",
    "        elpase_time = time.time() - start\n",
    "        print(f'runing time of opensearch_knn : {elpase_time}s seconds')\n",
    "        return get_topk_item(aos_knn_search(client, q_embedding, aos_index, size=10),2)\n",
    "         \n",
    "        \n",
    "   \n",
    "\n",
    "class CustomReverseIndexSearchTool(BaseTool):\n",
    "    name = \"custom_reverse_search\"\n",
    "    aos_client = OpenSearch(\n",
    "                hosts=[{'host': aos_endpoint, 'port': 443}],\n",
    "                http_auth = awsauth,\n",
    "                use_ssl=True,\n",
    "                verify_certs=True,\n",
    "                connection_class=RequestsHttpConnection)\n",
    "    aos_index=\"metadata_labels\"\n",
    "    \n",
    "    def _run(\n",
    "        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Opensearch 标签检索.\"\"\"\n",
    "        start = time.time()\n",
    "        opensearch_query_response = aos_reverse_search(aos_client, aos_index, \"doc\", query_input)\n",
    "        # logger.info(opensearch_query_response)\n",
    "        elpase_time = time.time() - start\n",
    "        logger.info(f'runing time of opensearch_query : {elpase_time}s seconds')\n",
    "        return get_topk_item(opensearch_query_response,2)\n",
    "        \n",
    "\n",
    "\n",
    "custom_tool_list=[]\n",
    "custom_tool_list.append(\n",
    "    Tool(\n",
    "        func=CustomReverseIndexSearchTool.run,\n",
    "        name=\"reverse index search\",\n",
    "        description=\"用于向量检索找到具体的数据库和表名\"\n",
    "    )  \n",
    ")\n",
    "\n",
    "custom_tool_list.append(\n",
    "    Tool(\n",
    "        func=CustomEmbeddingSearchTool.run,\n",
    "        name=\"embedding knn search\",\n",
    "        description=\"用于标签检索找到具体的数据库和表名\"\n",
    "    )    \n",
    ")\n",
    "\n",
    "db = SQLDatabase.from_uri(\n",
    "    \"mysql+pymysql://admin:******@database-us-west-2-demo.cluster-c1qvx9wzmmcz.us-west-2.rds.amazonaws.com/llm\",\n",
    "    sample_rows_in_table_info=0)\n",
    "\n",
    "sm_llm=SagemakerEndpoint(\n",
    "        #endpoint_name=\"chatglm-inference-0524-2023-06-01-07-11-27-379\",\n",
    "        endpoint_name=\"lmi-model-2023-09-15-03-45-27-834\",\n",
    "        region_name=\"us-west-2\", \n",
    "        model_kwargs=parameters,\n",
    "        content_handler=text_gen_content_handler3\n",
    ")\n",
    "\n",
    "\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=sm_llm)\n",
    "\n",
    "custom_suffix = \"\"\"\n",
    "我应该先利用标签检索，找到具体的数据库和表名，\n",
    "如果找不到，则利用向量检索查找，\n",
    "然后使用数据库工具查看我刚才找到的应该查询的库和表的详细schema元数据。\n",
    "\"\"\"\n",
    "agent = create_sql_agent(llm=llm,\n",
    "                         toolkit=toolkit,\n",
    "                         verbose=True,\n",
    "                         agent_type=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                         extra_tools=custom_tool_list,\n",
    "                         suffix=custom_suffix\n",
    "                        )\n",
    "\n",
    "agent.run(\"我需要知道销售报表中，下单金额最大的客户id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf8dfb-6487-4679-bd40-ec2d1d6693d7",
   "metadata": {},
   "source": [
    "* zero shot Agent test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0147d7fc-0b40-45f4-869e-213546026632",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.agents import ZeroShotAgent, Tool, AgentExecutor\n",
    "from langchain import OpenAI, LLMChain\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "chatglm_db_prompt_template = \"\"\"你是MySQL的专家。给定一个输入问题，创建一个语法正确的MySQL查询语句。\n",
    "除非用户在问题中指定了要获得的特定数量的示例，否则使用LIMIT子句查询最多{top_k}个结果。您可以对结果进行排序，以返回数据库中信息量最大的数据。您必须仅查询回答问题所需的列。将每个列名用反引号（`）括起来，表示为分隔的标识符。\n",
    "请注意，仅可以使用在{table_info}这些表中看到的列名，不要查询不存在的列。此外，还要注意哪个列在哪个表中。如果问题涉及”今天”，请注意使用CURDATE()函数获取当前日期.\n",
    "使用以下格式：\n",
    "Question:此处为问题\n",
    "SQLQuery:要运行的SQL查询\n",
    "SQLResult:SQLQuery的结果\n",
    "Answer:此处为最终答案\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "PROMPT_SUFFIX = \"\"\"Question:{input}\"\"\"\n",
    "\n",
    "chatglm_db_prompt = PromptTemplate(\n",
    "    input_variables=[\"input\", \"table_info\", \"top_k\"],\n",
    "    template=chatglm_db_prompt_template+PROMPT_SUFFIX,\n",
    ")\n",
    "\n",
    "\n",
    "#from langchain.chains.question_answering import load_qa_chain\n",
    "#chain = LLMChain(llm=sm_llm,prompt=chatglm_db_prompt)\n",
    "#chain.run({\"input\":\"我需要知道销售报表中，下单金额最大的客户id\",\"table_info\":\"'web_sales','customer'\",\"top_k\":\"3\"})\n",
    "\n",
    "\n",
    "#db_chain = CustomerizedSQLDatabaseChain.from_llm(llm=sm_llm, prompt=chatglm_db_prompt,db=db, verbose=True, top_k=3)\n",
    "db_chain = CustomerizedSQLDatabaseChain.from_llm(llm=sm_llm, db=db, verbose=True, top_k=3)\n",
    "db_chain.run(\"我需要知道销售报表中，下单金额最大的客户id\")\n",
    "#db_chain.run(\"帮我查下销售报表中，最大的销售金额\")\n",
    "\n",
    "#tools = []\n",
    "#prefix = \"\"\"尽你所能回答以下问题。您可以访问以下工具\"\"\"\n",
    "#suffix = \"\"\"开始! \n",
    "#\n",
    "#问题: {input}\n",
    "#{agent_scratchpad}\"\"\"\n",
    "#\n",
    "#prompt = ZeroShotAgent.create_prompt(\n",
    "#    tools, \n",
    "#    prefix=prefix, \n",
    "#    suffix=suffix, \n",
    "#    input_variables=[\"input\", \"agent_scratchpad\"]\n",
    "#)\n",
    "#\n",
    "##class AnalyzeInput(BaseModel):\n",
    "##    query: str = Field()\n",
    "#\n",
    "#print(prompt.template)\n",
    "#llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
    "#agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)\n",
    "#agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)\n",
    "#agent_executor.run(\"我需要知道llm数据库的web_sales销售报表中，ws_quantity和ws_ext_sales_price乘积最大的客户的c_customer_id\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p39",
   "language": "python",
   "name": "conda_pytorch_p39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
