{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1290f794-9477-4e45-ab20-81d04c040fc3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## langcain agent demo for ReAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf80354-7edd-4403-9c8b-97a4712f4dcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install langchain[all]\n",
    "!pip install langchain-experimental\n",
    "#!pip install sagemaker --upgrade\n",
    "!pip install  boto3\n",
    "!pip install requests_aws4auth\n",
    "!pip install opensearch-py\n",
    "!pip install pydantic==1.10.0\n",
    "#!pip install PyAthena[SQLAlchemy]==1.0.0\n",
    "#!pip install PyAthena[JDBC]==1.0.0\n",
    "#!pip install openai\n",
    "!pip install sqlalchemy-redshift\n",
    "!pip install redshift_connector\n",
    "!pip install SQLAlchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320a8478-64c7-4ccd-8bb7-4a9b0c71ab7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install SQLAlchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b216f11-a2b5-4f0b-b4de-060a1a64076a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## initial sagemaker env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f05414-d44b-464e-a0c7-46c317378ebd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "from typing import Dict\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3aed78-e14b-4d30-93d3-2871d6603bd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## intial lanchain lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aae9f94-a8df-49b5-b22b-f89e8d756b4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
    "from langchain.embeddings.sagemaker_endpoint import EmbeddingsContentHandler\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import ConversationBufferWindowMemory,ConversationBufferMemory\n",
    "from langchain import LLMChain\n",
    "from typing import Any, Dict, List, Union,Mapping, Optional, TypeVar, Union\n",
    "\n",
    "#os.environ[\"OPENAI_API_KEY\"]= \"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\"\n",
    "#from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "aos_endpoint=\"vpc-llm-rag-aos-seg3mzhpp76ncpxezdqtcsoiga.us-west-2.es.amazonaws.com\"\n",
    "region='us-west-2'\n",
    "username=\"admin\"\n",
    "passwd=\"(OL>0p;/\"\n",
    "index_name=\"metadata_index\"\n",
    "size=10\n",
    "\n",
    "### for sqlcoder\n",
    "class TextGenContentHandler2(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": model_kwargs\n",
    "            })\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        #print(response_json)\n",
    "        #sql_result=response_json[\"outputs\"].split(\"```sql\")[-1].split(\"```\")[0].split(\";\")[0].strip().replace(\"\\\\n\",\" \") + \";\"\n",
    "        sql_result=response_json[\"outputs\"]\n",
    "        return sql_result\n",
    "\n",
    "\n",
    "content_hander2=TextGenContentHandler2()\n",
    "\n",
    "## for embedding\n",
    "class EmbeddingContentHandler(EmbeddingsContentHandler):\n",
    "    parameters = {\n",
    "        \"max_new_tokens\": 50,\n",
    "        \"temperature\": 0,\n",
    "        \"min_length\": 10,\n",
    "        \"no_repeat_ngram_size\": 2,\n",
    "    }\n",
    "    def transform_input(self, inputs: list[str], model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\"inputs\": inputs, **model_kwargs})\n",
    "        return input_str.encode('utf-8')\n",
    "\n",
    "    def transform_output(self, output: bytes) -> List[List[float]]:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"sentence_embeddings\"]\n",
    "\n",
    "embedding_content_handler=EmbeddingContentHandler()\n",
    "    \n",
    "sm_embeddings = SagemakerEndpointEmbeddings(\n",
    "    # endpoint_name=\"endpoint-name\", \n",
    "    # credentials_profile_name=\"credentials-profile-name\", \n",
    "    #endpoint_name=\"huggingface-textembedding-bloom-7b1-fp1-2023-04-17-03-31-12-148\", \n",
    "    endpoint_name=\"st-paraphrase-mpnet-base-v2-2023-04-17-10-05-10-718-endpoint\",\n",
    "    region_name=\"us-west-2\", \n",
    "    content_handler=embedding_content_handler\n",
    ")\n",
    "\n",
    "\n",
    "parameters = {\n",
    "  \"max_new_tokens\": 300,\n",
    "  #\"no_repeat_ngram_size\": 2,\n",
    "}\n",
    "sm_sql_llm=SagemakerEndpoint(\n",
    "        endpoint_name=\"sqlcoder-2023-09-24-06-31-09-959-endpoint\",\n",
    "        region_name=\"us-west-2\", \n",
    "        model_kwargs=parameters,\n",
    "        content_handler=content_hander2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e9330b-0204-4e5a-bc7b-36ec7fd20818",
   "metadata": {
    "tags": []
   },
   "source": [
    "## func for agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0819cc2-5be1-4f78-8826-90c3ff2c72d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from requests_aws4auth import AWS4Auth\n",
    "import os\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from langchain.vectorstores import OpenSearchVectorSearch\n",
    "from langchain import PromptTemplate, SagemakerEndpoint\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.embeddings import SagemakerEndpointEmbeddings\n",
    "from langchain.llms.sagemaker_endpoint import ContentHandlerBase\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain import LLMChain\n",
    "\n",
    "\n",
    "\n",
    "def aos_knn_search(client, q_embedding, index, size=10):\n",
    "    query = {\n",
    "        \"size\": size,\n",
    "        \"query\": {\n",
    "            \"knn\": {\n",
    "                \"query_desc_embedding\": {\n",
    "                    \"vector\": q_embedding,\n",
    "                    \"k\": size\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    opensearch_knn_respose = []\n",
    "    query_response = client.search(\n",
    "        body=query,\n",
    "        index=index\n",
    "    )\n",
    "    opensearch_knn_respose = [{'idx':item['_source'].get('idx',1),'database_name':item['_source']['database_name'],'table_name':item['_source']['table_name'],'query_desc_text':item['query_desc_text'],\"score\":item[\"_score\"]}  for item in query_response[\"hits\"][\"hits\"]]\n",
    "    return opensearch_knn_respose\n",
    "\n",
    "def aos_reverse_search(client, index_name, field, query_term, exactly_match=False, size=10):\n",
    "    \"\"\"\n",
    "    search opensearch with query.\n",
    "    :param host: AOS endpoint\n",
    "    :param index_name: Target Index Name\n",
    "    :param field: search field\n",
    "    :param query_term: query term\n",
    "    :return: aos response json\n",
    "    \"\"\"\n",
    "    if not isinstance(client, OpenSearch):   \n",
    "        client = OpenSearch(\n",
    "            hosts=[{'host': client, 'port': 443}],\n",
    "            http_auth = awsauth,\n",
    "            use_ssl=True,\n",
    "            verify_certs=True,\n",
    "            connection_class=RequestsHttpConnection\n",
    "        )\n",
    "    query = None\n",
    "    if exactly_match:\n",
    "        query =  {\n",
    "            \"query\" : {\n",
    "                \"match_phrase\":{\n",
    "                    \"doc\": {\n",
    "                        \"query\": query_term,\n",
    "                        \"analyzer\": \"ik_smart\"\n",
    "                      }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    else:\n",
    "        query = {\n",
    "            \"size\": size,\n",
    "            \"query\": {\n",
    "                    \"bool\": {\n",
    "                            \"must\": [{\n",
    "                                \"term\": {\n",
    "                                         \"doc_type\": \"Question\"\n",
    "                                        }\n",
    "                                     },\n",
    "                                    {\n",
    "                                        \"match\": {\n",
    "                                            \"doc\": query_term\n",
    "                                        }\n",
    "                                    }\n",
    "                                ]\n",
    "                    },\n",
    "           \"sort\": [{\n",
    "               \"_score\": {\n",
    "                   \"order\": \"desc\"\n",
    "               }\n",
    "           }]\n",
    "        }\n",
    "    }        \n",
    "    query_response = client.search(\n",
    "        body=query,\n",
    "        index=index_name\n",
    "    )\n",
    "    \n",
    "    result_arr = [{'idx':item['_source'].get('idx',1),'database_name':item['_source']['database_name'],'table_name':item['_source']['table_name'],'query_desc_text':item['query_desc_text'],\"score\":item[\"_score\"]}  for item in query_response[\"hits\"][\"hits\"]]\n",
    "    return result_arr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_vector_by_sm_endpoint(questions, sm_client, endpoint_name):\n",
    "    parameters = {\n",
    "    }\n",
    "\n",
    "    response_model = sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(\n",
    "            {\n",
    "                \"inputs\": questions,\n",
    "                \"parameters\": parameters,\n",
    "                \"is_query\" : True,\n",
    "                \"instruction\" :  \"为这个句子生成表示以用于检索相关文章：\"\n",
    "            }\n",
    "        ),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "    json_str = response_model['Body'].read().decode('utf8')\n",
    "    json_obj = json.loads(json_str)\n",
    "    embeddings = json_obj['sentence_embeddings']\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_topk_items(opensearch_query_response, topk=5):\n",
    "    opensearch_knn_nodup = []\n",
    "    unique_ids = set()\n",
    "    for item in opensearch_query_response:\n",
    "        if item['id'] not in unique_ids:\n",
    "            opensearch_knn_nodup.append(item['score'], item['idx'],item['database_name'],item['table_name'],item['query_desc_text'])\n",
    "            unique_ids.add(item['id'])\n",
    "    return opensearch_knn_nodup\n",
    "\n",
    "\n",
    "\n",
    "def k_nn_ingestion_by_aos(docs,index,hostname,username,passwd):\n",
    "    auth = (username, passwd)\n",
    "    search = OpenSearch(\n",
    "        hosts = [{'host': aos_endpoint, 'port': 443}],\n",
    "        ##http_auth = awsauth ,\n",
    "        http_auth = auth ,\n",
    "        use_ssl = True,\n",
    "        verify_certs = True,\n",
    "        connection_class = RequestsHttpConnection\n",
    "    )\n",
    "    for doc in docs:\n",
    "        query_desc_embedding = doc['query_desc_embedding']\n",
    "        database_name = doc['database_name']\n",
    "        table_name = doc['table_name']\n",
    "        query_desc_text = doc[\"query_desc_text\"]\n",
    "        document = { \"query_desc_embedding\": query_desc_embedding, 'database_name':database_name, \"table_name\": table_name,\"query_desc_text\":query_desc_text}\n",
    "        search.index(index=index, body=document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa302e0a-b8ed-4fa0-934e-ab4ff0740316",
   "metadata": {
    "tags": []
   },
   "source": [
    "## major chain pipeline ################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ca53df-0f55-46ec-85a9-36316e25008b",
   "metadata": {},
   "source": [
    "### 0: index 创建"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6ccff8-4361-4f77-a42f-9d2c64ebe387",
   "metadata": {},
   "source": [
    "PUT metadata-index\n",
    "{\n",
    "    \"settings\" : {\n",
    "        \"index\":{\n",
    "            \"number_of_shards\" : 5,\n",
    "            \"number_of_replicas\" : 0,\n",
    "            \"knn\": \"true\",\n",
    "            \"knn.algo_param.ef_search\": 32\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"metadata_type\" : {\n",
    "                \"type\" : \"keyword\"\n",
    "            },\n",
    "            \"database_name\": {\n",
    "                \"type\" : \"keyword\"\n",
    "            },\n",
    "            \"table_name\": {\n",
    "               \"type\" : \"keyword\"\n",
    "            },\n",
    "            \"query_desc_text\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"ik_max_word\",\n",
    "                \"search_analyzer\": \"ik_smart\"\n",
    "            },\n",
    "            \"query_desc_embedding\": {\n",
    "                \"type\": \"knn_vector\",\n",
    "                \"dimension\": 768,\n",
    "                \"method\": {\n",
    "                    \"name\": \"hnsw\",\n",
    "                    \"space_type\": \"l2\",\n",
    "                    \"engine\": \"faiss\",\n",
    "                    \"parameters\": {\n",
    "                        \"ef_construction\": 512,\n",
    "                        \"m\": 32\n",
    "                    }\n",
    "                }            \n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ef73a-d691-4d30-b35e-1a5254cdb8d5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1: data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb9c564-9d44-4ba3-a279-34b6df2aa3c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_querys = \"\"\"最近一个月温度合格的派车单数量\"\"\"\n",
    "querys = all_querys.split(\"\\n\")\n",
    "\n",
    "all_tables = \"\"\"ads_bi_quality_monitor_shipping_detail\"\"\"\n",
    "tables=all_tables.split(\"\\n\")\n",
    "\n",
    "all_dbs = \"\"\"llm\"\"\"\n",
    "dbs=all_dbs.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8099c8-53e1-4581-9e87-381f5cf90020",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append(\"./code/\")\n",
    "#import func\n",
    "\n",
    "endpoint_name=\"bge-zh-15-2023-09-25-07-02-01-080-endpoint\"\n",
    "##########embedding by llm model##############\n",
    "sentense_vectors = []\n",
    "sentense_vectors=get_vector_by_sm_endpoint(querys,sm_client,endpoint_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793a20d5-fdd0-4be2-835d-8afc753062a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs=[]\n",
    "for index, sentence_vector in enumerate(sentense_vectors):\n",
    "    #print(index, sentence_vector)\n",
    "    doc = {\n",
    "        \"metadata_type\":\"table\",\n",
    "        \"database_name\":dbs[index],\n",
    "        \"table_name\": tables[index],\n",
    "        \"query_desc_text\":querys[index],\n",
    "        \"query_desc_embedding\": sentence_vector\n",
    "          }\n",
    "    docs.append(doc)\n",
    "\n",
    "#print(docs)\n",
    "23\n",
    "#########ingestion into aos ###################\n",
    "k_nn_ingestion_by_aos(docs,index_name,aos_endpoint,username,passwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b98b6e5-4ee6-4749-8aa5-c7c8a48b95ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2:自定义Agent ，定制context\n",
    "1:自定义AOS倒排及knn检索tools    \n",
    "2:自定义中文Sql Agent 的ReAct prompt 前缀   \n",
    "3:使用customerSqlDatabaseChain+Sql Agent触发   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beac4e51-32b7-460f-adee-b20151163e6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.tools.base import BaseTool, Tool, tool\n",
    "from langchain.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "from typing import Optional, Type\n",
    "from langchain.callbacks.manager import (\n",
    "    AsyncCallbackManagerForToolRun,\n",
    "    CallbackManagerForToolRun,\n",
    ")\n",
    "\n",
    "credentials = boto3.Session().get_credentials()\n",
    "region = boto3.Session().region_name\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, region, 'es', session_token=credentials.token)\n",
    "\n",
    "class CustomEmbeddingSearchTool(BaseTool):\n",
    "    name = \"custom_knn_search\"\n",
    "    aos_client = OpenSearch(\n",
    "                hosts=[{'host': aos_endpoint, 'port': 443}],\n",
    "                http_auth = awsauth,\n",
    "                use_ssl=True,\n",
    "                verify_certs=True,\n",
    "                connection_class=RequestsHttpConnection)\n",
    "    aos_index=\"metadata-index\"\n",
    "        \n",
    "    def _run(\n",
    "        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Opensearch 向量检索.\"\"\"\n",
    "        start = time.time()\n",
    "        query_embedding = get_vector_by_sm_endpoint(query, sm_client, sm_embeddings)\n",
    "        elpase_time = time.time() - start\n",
    "        print(f'runing time of opensearch_knn : {elpase_time}s seconds')\n",
    "        return get_topk_item(aos_knn_search(client, q_embedding, aos_index, size=10),2)\n",
    "         \n",
    "        \n",
    "   \n",
    "\n",
    "class CustomReverseIndexSearchTool(BaseTool):\n",
    "    name = \"custom_reverse_search\"\n",
    "    aos_client = OpenSearch(\n",
    "                hosts=[{'host': aos_endpoint, 'port': 443}],\n",
    "                http_auth = awsauth,\n",
    "                use_ssl=True,\n",
    "                verify_certs=True,\n",
    "                connection_class=RequestsHttpConnection)\n",
    "    aos_index=\"metadata_labels\"\n",
    "    \n",
    "    def _run(\n",
    "        self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None\n",
    "    ) -> str:\n",
    "        \"\"\"Opensearch 标签检索.\"\"\"\n",
    "        start = time.time()\n",
    "        opensearch_query_response = aos_reverse_search(aos_client, aos_index, \"doc\", query_input)\n",
    "        # logger.info(opensearch_query_response)\n",
    "        elpase_time = time.time() - start\n",
    "        logger.info(f'runing time of opensearch_query : {elpase_time}s seconds')\n",
    "        return get_topk_item(opensearch_query_response,2)\n",
    "        \n",
    "\n",
    "\n",
    "custom_tool_list=[]\n",
    "custom_tool_list.append(\n",
    "    Tool(\n",
    "        func=CustomReverseIndexSearchTool.run,\n",
    "        name=\"reverse index search\",\n",
    "        description=\"用于向量检索找到具体的数据库和表名\"\n",
    "    )  \n",
    ")\n",
    "\n",
    "custom_tool_list.append(\n",
    "    Tool(\n",
    "        func=CustomEmbeddingSearchTool.run,\n",
    "        name=\"embedding knn search\",\n",
    "        description=\"用于标签检索找到具体的数据库和表名\"\n",
    "    )    \n",
    ")\n",
    "\n",
    "db = SQLDatabase.from_uri(\n",
    "    \"mysql+pymysql://admin:admin12345678@database-us-west-2-demo.cluster-c1qvx9wzmmcz.us-west-2.rds.amazonaws.com/llm\",\n",
    "    sample_rows_in_table_info=0)\n",
    "\n",
    "\n",
    "### for llama2\n",
    "class TextGenContentHandler(LLMContentHandler):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def transform_input(self, prompt: str, model_kwargs: Dict) -> bytes:\n",
    "        input_str = json.dumps({\n",
    "                \"inputs\": prompt,\n",
    "                \"parameters\": model_kwargs\n",
    "            })\n",
    "        return input_str.encode('utf-8')\n",
    "    \n",
    "    def transform_output(self, output: bytes) -> str:\n",
    "        response_json = json.loads(output.read().decode(\"utf-8\"))\n",
    "        return response_json[\"outputs\"]\n",
    "\n",
    "\n",
    "content_hander=TextGenContentHandler()\n",
    "\n",
    "sm_llm=SagemakerEndpoint(\n",
    "        endpoint_name=\"lmi-model-2023-09-25-09-56-14-425\",\n",
    "        region_name=\"us-west-2\", \n",
    "        model_kwargs=parameters,\n",
    "        content_handler=content_hander\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=sm_llm)\n",
    "\n",
    "custom_suffix = \"\"\"\n",
    "我应该先利用标签检索，找到具体的数据库和表名，\n",
    "如果找不到，则利用向量检索查找，\n",
    "然后使用数据库工具查看我刚才找到的应该查询的库和表的详细schema元数据。\n",
    "\"\"\"\n",
    "agent = create_sql_agent(llm=llm,\n",
    "                         toolkit=toolkit,\n",
    "                         verbose=True,\n",
    "                         agent_type=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "                         extra_tools=custom_tool_list,\n",
    "                         suffix=custom_suffix\n",
    "                        )\n",
    "\n",
    "agent.run(\"我需要知道销售报表中，下单金额最大的客户id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf8dfb-6487-4679-bd40-ec2d1d6693d7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## llm_chain自定义 Agent test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6fb3ec-2eb0-483a-8c88-1bbd06781488",
   "metadata": {
    "tags": []
   },
   "source": [
    "* 使用llm chain做chat的planner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c9e6f8-95fe-42ec-8ddb-b6e0658676ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from typing import Any, Dict, List, Optional\n",
    "from pydantic import Extra\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.prompts.base import BasePromptTemplate\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_experimental.plan_and_execute import PlanAndExecute, load_agent_executor, load_chat_planner\n",
    "from langchain.agents.tools import Tool\n",
    "\n",
    "\n",
    "sql_prompt_template = \"\"\"\n",
    "You are a MySQL expert. Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer to the input question.\n",
    "Unless the user specifies in the question a specific number of examples to obtain, query for at most 5 results using the LIMIT clause as per MySQL. \n",
    "You can order the results to return the most informative data in the database.\n",
    "Never query for all columns from a table. You must query only the columns that are needed to answer the question. Wrap each column name in backticks (`) to denote them as delimited identifiers.\n",
    "Pay attention to use only the column names you can see in the tables below. Be careful to not query for columns that do not exist. Also, pay attention to which column is in which table.\n",
    "Pay attention to use CURDATE() function to get the current date, if the question involves \"today\".\n",
    "\n",
    "Use the following format:\n",
    "Question: Question here\n",
    "SQLQuery: SQL Query to run\n",
    "SQLResult: Result of the SQLQuery\n",
    "Answer: Final answer here\n",
    "\n",
    "Only use the following tables:\n",
    "{table_info}\n",
    "\n",
    "Question: {question}\n",
    "SQLQuery:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "prompt = PromptTemplate(\n",
    "        template=sql_prompt_template, input_variables=[\"table_info\", \"question\",\"dialect\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def build_chat_chain():\n",
    "    conversation_with_summary_chain = ConversationChain(\n",
    "        llm=sm_llm, \n",
    "        memory=memory,\n",
    "        verbose=True\n",
    "    )\n",
    "    return conversation_with_summary_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6061ce2b-f0e7-4f69-85fb-ae4bb0dac503",
   "metadata": {},
   "source": [
    "* 使用SqlDatabaseChain作为db tools做数据库交互\n",
    "* 将db tools加入之前定义的元数据召回的tools列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ef2532-b626-4052-a3ab-f4a32ff59cc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_query(query):\n",
    "    PROMPT_sql = PromptTemplate(\n",
    "        input_variables=[\"question\", \"table_info\", \"dialect\"], template=sql_prompt_template\n",
    "    )\n",
    "    db_chain = SQLDatabaseChain.from_llm(sm_sql_llm, db, prompt=PROMPT_sql, verbose=True, return_intermediate_steps=False)\n",
    "    response=db_chain.run(query)\n",
    "    return response\n",
    "\n",
    "custom_tool_list.append(\n",
    "   Tool(\n",
    "        name=\"Db Querying Tool\",\n",
    "        func=run_query,\n",
    "        description=\"\"\"用于连接数据库获取schema元数据信息\"\"\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea0f676-5fee-40a7-a1e1-71aef0af09cc",
   "metadata": {},
   "source": [
    "* 使用llm chain做chat的planner\n",
    "* 使用langchain plannerAndExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b00cf-8fcd-445f-82db-4b0d019096a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "combo_template = \"\"\"\n",
    "让我们先了解问题并制定解决问题的计划。\n",
    "请输出以标题“plan:”开头的计划，然后输出步骤的编号列表。在计划步骤时，不要使用过去的对话历史记录。\n",
    "请将计划制定为准确完成任务所需的最少步骤数。\n",
    "\n",
    "以下是关于使用工具解决任务的指导，请严格遵守：\n",
    "先利用\"reverse index search\" 标签检索工具\n",
    "如果结果为空，则利用\"embedding knn search\"向量检索工具查找\n",
    "然后使用\"db Querying Tool\"数据库工具查看我刚才找到的应该查询的库和表的详细schema元数据。\n",
    "\"\"\"\n",
    "planner = load_chat_planner(sm_llm)\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(combo_template)\n",
    "human_message_prompt = planner.llm_chain.prompt.messages[1]\n",
    "planner.llm_chain.prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "executor = load_agent_executor(sm_llm, custom_tool_list, verbose=True)\n",
    "agent = PlanAndExecute(planner=planner, executor=executor, verbose=True, max_iterations=2,memory=memory)\n",
    "\n",
    "\n",
    "output = agent(\"最近一个月温度合格的派车单数量\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db1477b-b796-42f1-b810-3238280b44b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
