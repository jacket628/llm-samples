{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c37930da-e0a6-406c-8a6a-a062746c7077",
   "metadata": {},
   "source": [
    "# An sample to finetune Baichuan2-7B-Base distruibutely on SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cb561e-2329-42bd-b280-119a8f06f6fe",
   "metadata": {},
   "source": [
    "**To avoid 'no left space' error, move the docker root directory**\n",
    "\n",
    "sudo systemctl stop docker\n",
    "\n",
    "sudo systemctl stop docker.socket \n",
    "\n",
    "sudo mv /var/lib/docker /home/ec2-user/SageMaker \n",
    "\n",
    "sudo ln -s /home/ec2-user/SageMaker/docker /var/lib/docker \n",
    "\n",
    "sudo systemctl start docker.socket\n",
    "\n",
    "sudo systemctl start docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eedcfaa-83cf-4645-a5d3-8eaa5a0dae30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Update sagemaker python sdk version\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df8dbba-4382-4fe5-9e6d-b75e1ca72bd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "sess                     = sagemaker.Session()\n",
    "role                     = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "\n",
    "account                  = sess.boto_session.client(\"sts\").get_caller_identity()[\"Account\"]\n",
    "region                   = sess.boto_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f07aa5a-b488-4294-8262-83023fc75adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script bash\n",
    "\n",
    "rm -rf s5cmd*\n",
    "wget https://github.com/peak/s5cmd/releases/download/v2.1.0/s5cmd_2.1.0_Linux-64bit.tar.gz\n",
    "tar xvzf s5cmd_2.1.0_Linux-64bit.tar.gz\n",
    "\n",
    "rm -rf src\n",
    "mkdir src\n",
    "cp s5cmd src/\n",
    "cd src\n",
    "\n",
    "# The version is 2023.09.09\n",
    "git clone https://github.com/baichuan-inc/Baichuan2.git\n",
    "cd Baichuan2\n",
    "git reset --hard f620769c667ce6380770398af1d8449e775ec271"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3ee865-66c0-4897-843d-194b4cec10e3",
   "metadata": {},
   "source": [
    "## Download pretrained model from HuggingFace Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b62370-89f7-4d8f-b5ee-a83781c88326",
   "metadata": {},
   "source": [
    "To avoid download model from Huggingface hub failure, we download first and push those model files to S3 bucket first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4f4ec6-3033-4cb3-845a-1f2e1fc0a080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ac9937-a3f8-49fc-94ad-faadfb7752c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "local_cache_path = Path(\"./model\")\n",
    "local_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = \"baichuan-inc/Baichuan2-7B-Base\"\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.model\", \"*.py\", \"*.txt\"]\n",
    "\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_cache_path,\n",
    "    allow_patterns=allow_patterns,\n",
    "    revision='f2cc3a689c5eba7dc7fd3757d0175d312d167604'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c20ac2-b016-4edd-81eb-d0f865f9234c",
   "metadata": {},
   "source": [
    "**Upload model files to S3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e753bc-1f5d-4e5a-8994-b079e267fa6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the model files path\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "local_model_path = None\n",
    "\n",
    "paths = os.walk(r'./model')\n",
    "for root, dirs, files in paths:\n",
    "    for file in files:\n",
    "        if file == 'config.json':\n",
    "            print(os.path.join(root,file))\n",
    "            local_model_path = str(os.path.join(root,file))[0:-11]\n",
    "            print(local_model_path)\n",
    "if local_model_path == None:\n",
    "    print(\"Model download may failed, please check prior step!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f0e690-fa83-4952-81cf-78aa3d173ca4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script env sagemaker_default_bucket=$sagemaker_default_bucket local_model_path=$local_model_path bash\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync ${local_model_path} s3://${sagemaker_default_bucket}/llm/models/baichuan2/baichuan-inc/Baichuan2-7B-Base/ \n",
    "\n",
    "rm -rf model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b76be42-cc9b-48dd-8d12-ab60d5c2d3e2",
   "metadata": {},
   "source": [
    "## Prepare docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23458d2c-eb6a-4e94-a56e-32392e9dd699",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile Dockerfile\n",
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "# From 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:1.13.1-transformers4.26.0-gpu-py39-cu117-ubuntu20.04 \n",
    "From 763104351884.dkr.ecr.us-west-2.amazonaws.com/huggingface-pytorch-training:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04\n",
    "\n",
    "ENV LANG=C.UTF-8\n",
    "ENV PYTHONUNBUFFERED=TRUE\n",
    "ENV PYTHONDONTWRITEBYTECODE=TRUE\n",
    "\n",
    "RUN pip3 install deepspeed==0.10.0 \\\n",
    "    && pip3 install transformers==4.29.2 \\\n",
    "    && pip3 install jsonlines==3.1.0 \\\n",
    "    && pip3 install accelerate==0.22.0 \\\n",
    "    && pip3 install xformers==0.0.21 \\\n",
    "    && pip3 install peft==0.5.0\n",
    "\n",
    "COPY src/Baichuan2/fine-tune/requirements.txt .\n",
    "RUN pip3 install -r requirements.txt\n",
    "\n",
    "## Make all local GPUs visible\n",
    "ENV NVIDIA_VISIBLE_DEVICES=\"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5d016e-2b6b-453d-a4a5-84062d2aed0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## You should change below region code to the region you used, here sample is use us-west-2\n",
    "!aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 763104351884.dkr.ecr.us-west-2.amazonaws.com"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf2e4ea-4ae9-473d-92d8-e11a56c1d499",
   "metadata": {},
   "source": [
    "**Build image and push to ECR.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075cd597-cf50-4c90-a2ac-110b72c9f202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## define repo name, should contain *sagemaker* in the name\n",
    "repo_name = \"sagemaker-baichuan2-7b-base-finetune\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8c4c49-f234-4356-802c-616b79e3975e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script env repo_name=$repo_name bash\n",
    "\n",
    "#!/usr/bin/env bash\n",
    "\n",
    "# This script shows how to build the Docker image and push it to ECR to be ready for use\n",
    "# by SageMaker.\n",
    "\n",
    "# The argument to this script is the image name. This will be used as the image on the local\n",
    "# machine and combined with the account and region to form the repository name for ECR.\n",
    "# The name of our algorithm\n",
    "algorithm_name=${repo_name}\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "aws ecr get-login-password --region ${region}|docker login --username AWS --password-stdin ${fullname}\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build -t ${algorithm_name} .\n",
    "docker tag ${algorithm_name} ${fullname}\n",
    "\n",
    "docker push ${fullname}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eb15e9-107b-46cf-9644-ad7bc4c3231d",
   "metadata": {},
   "source": [
    "**Generate training entrypoint script.**\n",
    "\n",
    "**Note: DO NOT CHANGE BELOW VAlUE OF \"output_dir\" and \"cache_dir\", keep it \"/tmp/llama_out\" and \"/tmp\".**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a49aa83-316b-45aa-931e-f18dbe003766",
   "metadata": {},
   "source": [
    "Below is just a testing to fine-tune on a sample dataset (just 8 samples), you could change ```data_path``` to your dataset for furthur fine tune.\n",
    "\n",
    "For the dataset download, you could follow the way how to download pretrain model:\n",
    "```\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/llm/models/llama/pinkmanlove/llama-7b-hf/* /tmp/llama_pretrain/\n",
    "```\n",
    "\n",
    "It is recommend to use the folder ```/tmp/dataset/```."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f7879c-074c-4537-ba40-ee8d41a6747c",
   "metadata": {},
   "source": [
    "## Notice\n",
    "\n",
    "We modified some parts of ```Baichuan2/fine-tune/fine-tune.py```, such as how to save model.\n",
    "\n",
    "The version is 2023-09-09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7330e88-dc4b-4fdf-a2a5-bd560b3334a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mv src/Baichuan2/fine-tune/fine-tune.py src/Baichuan2/fine-tune/fine-tune.py.bk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b21e47-9de7-4c59-a47d-0032ce697891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile src/Baichuan2/fine-tune/fine-tune.py\n",
    "\n",
    "import os\n",
    "import math\n",
    "import pathlib\n",
    "from typing import Optional, Dict\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import transformers\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelArguments:\n",
    "    model_name_or_path: Optional[str] = field(default=\"baichuan-inc/Baichuan2-7B-Base\")\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataArguments:\n",
    "    data_path: str = field(\n",
    "        default=None, metadata={\"help\": \"Path to the training data.\"}\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainingArguments(transformers.TrainingArguments):\n",
    "    cache_dir: Optional[str] = field(default=None)\n",
    "    optim: str = field(default=\"adamw_torch\")\n",
    "    model_max_length: int = field(\n",
    "        default=512,\n",
    "        metadata={\n",
    "            \"help\": \"Maximum sequence length. Sequences will be right padded (and possibly truncated).\"\n",
    "        },\n",
    "    )\n",
    "    use_lora: bool = field(default=False)\n",
    "\n",
    "\n",
    "class SupervisedDataset(Dataset):\n",
    "    \"\"\"Dataset for supervised fine-tuning.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        tokenizer,\n",
    "        model_max_length,\n",
    "        user_tokens=[195],\n",
    "        assistant_tokens=[196],\n",
    "    ):\n",
    "        super(SupervisedDataset, self).__init__()\n",
    "        self.data = json.load(open(data_path))\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model_max_length = model_max_length\n",
    "        self.user_tokens = user_tokens\n",
    "        self.assistant_tokens = assistant_tokens\n",
    "        self.ignore_index = -100\n",
    "        item = self.preprocessing(self.data[0])\n",
    "        print(\"input:\", self.tokenizer.decode(item[\"input_ids\"]))\n",
    "        labels = []\n",
    "        for id_ in item[\"labels\"]:\n",
    "            if id_ == -100:\n",
    "                continue\n",
    "\n",
    "            labels.append(id_)\n",
    "        print(\"label:\", self.tokenizer.decode(labels))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def preprocessing(self, example):\n",
    "        input_ids = []\n",
    "        labels = []\n",
    "\n",
    "        for message in example[\"conversations\"]:\n",
    "            from_ = message[\"from\"]\n",
    "            value = message[\"value\"]\n",
    "            value_ids = self.tokenizer.encode(value)\n",
    "\n",
    "            if from_ == \"human\":\n",
    "                input_ids += self.user_tokens + value_ids\n",
    "                labels += [self.tokenizer.eos_token_id] + [self.ignore_index] * len(\n",
    "                    value_ids\n",
    "                )\n",
    "            else:\n",
    "                input_ids += self.assistant_tokens + value_ids\n",
    "                labels += [self.ignore_index] + value_ids\n",
    "        input_ids.append(self.tokenizer.eos_token_id)\n",
    "        labels.append(self.tokenizer.eos_token_id)\n",
    "        input_ids = input_ids[: self.model_max_length]\n",
    "        labels = labels[: self.model_max_length]\n",
    "        input_ids += [self.tokenizer.pad_token_id] * (\n",
    "            self.model_max_length - len(input_ids)\n",
    "        )\n",
    "        labels += [self.ignore_index] * (self.model_max_length - len(labels))\n",
    "        input_ids = torch.LongTensor(input_ids)\n",
    "        labels = torch.LongTensor(labels)\n",
    "        attention_mask = input_ids.ne(self.tokenizer.pad_token_id)\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"labels\": labels,\n",
    "            \"attention_mask\": attention_mask,\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx) -> Dict[str, torch.Tensor]:\n",
    "        return self.preprocessing(self.data[idx])\n",
    "\n",
    "\n",
    "def train():\n",
    "    parser = transformers.HfArgumentParser(\n",
    "        (ModelArguments, DataArguments, TrainingArguments)\n",
    "    )\n",
    "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
    "\n",
    "    model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        trust_remote_code=True,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "    )\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        use_fast=False,\n",
    "        trust_remote_code=True,\n",
    "        model_max_length=training_args.model_max_length,\n",
    "        cache_dir=training_args.cache_dir,\n",
    "    )\n",
    "    if training_args.use_lora:\n",
    "        from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "        peft_config = LoraConfig(\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "            target_modules=[\"W_pack\"],\n",
    "            inference_mode=False,\n",
    "            r=1,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1,\n",
    "        )\n",
    "        model.enable_input_require_grads()\n",
    "        model = get_peft_model(model, peft_config)\n",
    "        model.print_trainable_parameters()\n",
    "\n",
    "    dataset = SupervisedDataset(\n",
    "        data_args.data_path, tokenizer, training_args.model_max_length\n",
    "    )\n",
    "    trainer = transformers.Trainer(\n",
    "        model=model, args=training_args, train_dataset=dataset, tokenizer=tokenizer\n",
    "    )\n",
    "    trainer.train()\n",
    "    # trainer.save_state()\n",
    "    # trainer.save_model(output_dir=training_args.output_dir)\n",
    "    \n",
    "    tokenizer.save_pretrained(training_args.output_dir)\n",
    "    trainer.save_model(training_args.output_dir)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86395940-1bda-4cb8-9465-60bf38bef57a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./src/ds-train-dist.sh\n",
    "#!/bin/bash\n",
    "CURRENT_HOST=\"${SM_CURRENT_HOST}\"\n",
    "\n",
    "\n",
    "IFS=',' read -ra hosts_array <<< \"${SM_HOSTS}\"\n",
    "NNODES=${#hosts_array[@]}\n",
    "NODE_RANK=0\n",
    "\n",
    "for i in \"${!hosts_array[@]}\"; do\n",
    "    if [[ \"${hosts_array[$i]}\" == *${CURRENT_HOST}* ]]; then\n",
    "        echo \"host index：$i\"\n",
    "        NODE_RANK=\"$i\" \n",
    "    fi\n",
    "done\n",
    "   \n",
    "    \n",
    "MASTER_PORT=\"13579\"\n",
    "export NCCL_SOCKET_IFNAME=\"eth0\"\n",
    "\n",
    "#Configure the distributed arguments for torch.distributed.launch.\n",
    "GPUS_PER_NODE=\"$SM_NUM_GPUS\"\n",
    "DISTRIBUTED_ARGS=\"--nproc_per_node $GPUS_PER_NODE \\\n",
    "                  --nnodes $NNODES \\\n",
    "                  --node_rank $NODE_RANK \\\n",
    "                  --master_addr $MASTER_ADDR \\\n",
    "                  --master_port $MASTER_PORT\"\n",
    "\n",
    "chmod +x ./s5cmd\n",
    "./s5cmd sync s3://$MODEL_S3_BUCKET/llm/models/baichuan2/baichuan-inc/Baichuan2-7B-Base/* /tmp/baichuan2/\n",
    "\n",
    "    \n",
    "DEEPSPEED_OPTS=\"\"\"\n",
    "    Baichuan2/fine-tune/fine-tune.py \n",
    "    --deepspeed Baichuan2/fine-tune/ds_config.json \n",
    "    --model_name_or_path \"/tmp/baichuan2/\" \n",
    "    --data_path \"Baichuan2/fine-tune/data/belle_chat_ramdon_10k.json\" \n",
    "    --output_dir \"/tmp/baichuan2_out\" \n",
    "    --num_train_epochs 1 \n",
    "    --per_device_train_batch_size 1 \n",
    "    --per_device_eval_batch_size  1 \n",
    "    --gradient_accumulation_steps 1 \n",
    "    --evaluation_strategy \"no\" \n",
    "    --save_strategy \"steps\" \n",
    "    --save_steps 2000 \n",
    "    --save_total_limit 2 \n",
    "    --learning_rate 2e-5 \n",
    "    --adam_beta1 0.9 \n",
    "    --adam_beta2 0.98 \n",
    "    --adam_epsilon 1e-8 \n",
    "    --weight_decay 1e-4 \n",
    "    --max_grad_norm 1.0 \n",
    "    --warmup_ratio 0.0 \n",
    "    --lr_scheduler_type \"constant\" \n",
    "    --logging_steps 1 \n",
    "    --cache_dir '/tmp' \n",
    "    --model_max_length 512 \n",
    "    --gradient_checkpointing True \n",
    "    --bf16 True \n",
    "    --tf32 True \n",
    "    --report_to \"none\"\n",
    "\"\"\"    \n",
    "\n",
    "CMD=\"torchrun ${DISTRIBUTED_ARGS} ${DEEPSPEED_OPTS}\"\n",
    "echo ${CMD}\n",
    "${CMD} 2>&1 \n",
    "\n",
    "if [[ \"${CURRENT_HOST}\" == \"${MASTER_ADDR}\" ]]; then  \n",
    "    ./s5cmd sync /tmp/baichuan2_out s3://$MODEL_S3_BUCKET/llm/models/baichuan2/output/baichuan-inc/Baichuan-7B-Base/$(date +%Y-%m-%d-%H-%M-%S)/\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74742cbf-3de0-4221-b09d-24559370c65d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## The image uri which is build and pushed above\n",
    "image_uri = \"{}.dkr.ecr.{}.amazonaws.com/{}:latest\".format(account, region, repo_name)\n",
    "image_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ef0432-8dd5-4113-997b-50211448f654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "\n",
    "environment = {\n",
    "    'MODEL_S3_BUCKET': sagemaker_default_bucket # The bucket to store pretrained model and fine-tune model\n",
    "}\n",
    "\n",
    "base_job_name = 'baichuan2-7b-base-finetune'\n",
    "\n",
    "instance_type = 'ml.p4d.24xlarge'\n",
    "# instance_type = 'ml.g5.12xlarge'\n",
    "\n",
    "estimator = Estimator(role=role,\n",
    "                      entry_point='ds-train-dist.sh',\n",
    "                      source_dir='./src',\n",
    "                      base_job_name=base_job_name,\n",
    "                      instance_count=1,\n",
    "                      instance_type=instance_type,\n",
    "                      image_uri=image_uri,\n",
    "                      environment=environment,\n",
    "                      disable_profiler=True,\n",
    "                      debugger_hook_config=False)\n",
    "\n",
    "\n",
    "# estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf5f8bd-fedc-4bba-9874-909195b762e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a6f7d2-80b1-4332-81f5-65e8336910c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35aceef6-e0f4-4d12-8546-d0793db373e7",
   "metadata": {},
   "source": [
    "\n",
    "### Serve large models on SageMaker with DJL DeepSpeed Container\n",
    "\n",
    "In this notebook, we explore how to host a large language model on SageMaker using the latest container launched using from DeepSpeed and DJL. DJL provides for the serving framework while DeepSpeed is the key sharding library we leverage to enable hosting of large models.We use DJLServing as the model serving solution in this example. DJLServing is a high-performance universal model serving solution powered by the Deep Java Library (DJL) that is programming language agnostic. To learn more about DJL and DJLServing, you can refer to our recent blog post (https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/).\n",
    "\n",
    "Model parallelism can help deploy large models that would normally be too large for a single GPU. With model parallelism, we partition and distribute a model across multiple GPUs. Each GPU holds a different part of the model, resolving the memory capacity issue for the largest deep learning models with billions of parameters. This notebook uses tensor parallelism techniques which allow GPUs to work simultaneously on the same layer of a model and achieve low latency inference relative to a pipeline parallel solution.\n",
    "\n",
    "SageMaker has rolled out DeepSpeed container which now provides users with the ability to leverage the managed serving capabilities and help to provide the un-differentiated heavy lifting.\n",
    "\n",
    "In this notebook, we deploy the open source llama 7B model across GPU's on a ml.g5.48xlarge instance. Note that the llama 7B fp16 model can be deployed on single GPU such as g5.2xlarge (24GB VRAM), we jsut show the code which can deploy the llm accross multiple GPUs in SageMaker. DeepSpeed is used for tensor parallelism inference while DJLServing handles inference requests and the distributed workers. For further reading on DeepSpeed you can refer to https://arxiv.org/pdf/2207.00032.pdf \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcedb008-79dd-4c20-9509-53a374a2a478",
   "metadata": {},
   "source": [
    "## Create SageMaker compatible Model artifact and Upload Model to S3\n",
    "\n",
    "SageMaker needs the model to be in a Tarball format. In this notebook we are going to create the model with the Inference code to shorten the end point creation time. \n",
    "\n",
    "The tarball is in the following format\n",
    "\n",
    "```\n",
    "code\n",
    "├──── \n",
    "│   └── model.py\n",
    "│   └── requirements.txt\n",
    "│   └── serving.properties\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "- `model.py` is the key file which will handle any requests for serving. \n",
    "- `requirements.txt` has the required libraries needed to be installed when the container starts up.\n",
    "- `serving.properties` is the script that will have environment variables which can be used to customize model.py at run time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07349695-9935-4d1e-bebf-22a8d95623c0",
   "metadata": {},
   "source": [
    "#### Serving.properties has engine parameter which tells the DJL model server to use the DeepSpeed engine to load the model.\n",
    "\n",
    "option.tensor_parallel_degree:  if we use the g5.48xlarge which has 8 GPUs, so we set the tensor_parallel_degree to 8.\n",
    "\n",
    "option.s3url:  you should use your model path here. And the s3 path must be ended with \"/\".\n",
    "\n",
    "batch_size:   it is for server side batch based on request level. You can set batch_size to the large value which can not result in the OOM. The current code about model.py is just demo for one prompt per client request.\n",
    "\n",
    "max_batch_delay:   it is counted by millisecond. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf83dcf-c882-4d60-b75b-95141cafb804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf src\n",
    "!mkdir src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba77005d-396c-4030-a12a-e87bde01bb83",
   "metadata": {},
   "source": [
    "**Copy python files from original model to finetuned model path**\n",
    "\n",
    "for examples:\n",
    "\n",
    "!aws s3 cp s3://sagemaker-us-west-2-928808346782/llm/models/baichuan2/baichuan-inc/Baichuan2-7B-Base/ s3://sagemaker-us-west-2-928808346782/llm/models/baichuan/output/baichuan-inc/Baichuan-7B/2023-08-31-06-50-39/baichuan_out/ --recursive --exclude \"*\" --include \"*.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b18efd-fe51-4133-a655-ba4b338adf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy original *.py files for finetuned model\n",
    "!aws s3 cp ${s3_url_original_model} ${s3_url_finetuned_model} --recursive --exclude \"*\" --include \"*.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd206b4-6cb7-44a2-b9b5-36f57e8dfa8b",
   "metadata": {},
   "source": [
    "**Copy finetune cell output above, modify 'option.s3url' value**\n",
    "\n",
    "for example, option.s3url=s3://sagemaker-us-west-2-928808346782/llm/models/llama2/output/TheBloke/Llama-2-13B-fp16/2023-08-10-05-32-33/llama_out/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91feb354-4023-40ce-83a8-3c683c83a8e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./src/serving.properties\n",
    "engine=DeepSpeed\n",
    "option.tensor_parallel_degree=1\n",
    "option.s3url=${option.s3url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10af817-98d3-41d1-8725-00246ca42668",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./src/requirements.txt\n",
    "transformers==4.29.2\n",
    "sagemaker\n",
    "nvgpu\n",
    "xformers\n",
    "peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a727878-67c1-4e2f-8b5f-e059d10e54bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./src/model.py\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import deepspeed\n",
    "import transformers\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "generator = None\n",
    "\n",
    "\n",
    "def load_model(properties):\n",
    "    tensor_parallel = properties[\"tensor_parallel_degree\"]\n",
    "    model_location = properties['model_dir']\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties['model_id']\n",
    "    logging.info(f\"Loading model in {model_location}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_location, torch_dtype=torch.float16, use_fast=False, trust_remote_code=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_location, device_map=\"auto\", torch_dtype=torch.float16, trust_remote_code=True)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    " \n",
    "def handle(inputs: Input) -> None:\n",
    "    global model, tokenizer\n",
    "    \n",
    "    try:\n",
    "        if not model:\n",
    "            model, tokenizer = load_model(inputs.get_properties())\n",
    "\n",
    "        if inputs.is_empty():\n",
    "            return None\n",
    "\n",
    "        data = inputs.get_as_json()    \n",
    "\n",
    "        input_data = data[\"inputs\"]\n",
    "        params = data[\"parameters\"]\n",
    "\n",
    "        logging.info(f\"data               : {data}\")\n",
    "        logging.info(f\"input_data         : {input_data}\")\n",
    "        logging.info(f\"params             : {params}\")\n",
    "        \n",
    "        inputs_tokenized = tokenizer(input_data, return_tensors='pt')\n",
    "        inputs_tokenized = inputs_tokenized.to('cuda:0')\n",
    "        logging.info(f\"inputs_tokenized   : {inputs_tokenized}\")\n",
    "\n",
    "        pred = model.generate(**inputs_tokenized, **params)\n",
    "        logging.info(f\"pred               : {pred}\")\n",
    "        \n",
    "        response = tokenizer.decode(pred.cpu()[0], skip_special_tokens=True)\n",
    "        # response = tokenizer.decode(pred[0], skip_special_tokens=True)\n",
    "        logging.info(f\"response           : {response}\")\n",
    "\n",
    "        result = {\"outputs\": response}\n",
    "        logging.info(f\"result             : {result}\")\n",
    "\n",
    "        return Output().add({\"code\":0,\"msg\":\"ok\",\"data\":result})\n",
    "    except Exception as e:\n",
    "        return Output().add_as_json({\"code\":-1,\"msg\":e})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff846e6-2a3b-4305-a77e-d7797770297f",
   "metadata": {},
   "source": [
    "#### Create required variables and initialize them to create the endpoint, we leverage boto3 for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6727e6d5-074f-4ac6-b553-b4a97f62056a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "sage_session = sagemaker.Session()\n",
    "model_bucket = sage_session.default_bucket()  # bucket to house artifacts\n",
    "s3_code_prefix = (\n",
    "    \"llm/models/baichuan2/code-7b-base\"  # folder within bucket where code artifact will go\n",
    ")\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1ed5da-49db-44c0-8f1f-2a416b6c4de9",
   "metadata": {},
   "source": [
    "**Image URI for the DJL container is being used here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf1fdc1-1718-41d1-b06e-44075a51ceea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Note that: you can modify the image url according to your specific region.\n",
    "# inference_image_uri = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.21.0-deepspeed0.8.0-cu117\"\n",
    "\n",
    "inference_image_uri = \"763104351884.dkr.ecr.us-west-2.amazonaws.com/djl-inference:0.23.0-deepspeed0.9.5-cu118\"\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a87eb4-0b3b-4199-8438-55f160d19b64",
   "metadata": {},
   "source": [
    "**Create the Tarball and then upload to S3 location**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44652f16-8321-4b1a-a9bd-143eb80ff0cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -f model.tar.gz\n",
    "!tar czvf model.tar.gz src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb8c4d-b4fd-444f-951c-25b7f01dc9a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_artifact = sage_session.upload_data(\"model.tar.gz\", model_bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {s3_code_artifact}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580bb4e8-ca46-49da-a1d3-59fbc930f250",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"S3 Model Bucket is -- > {model_bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280b9914-3879-4839-a568-322d66bd0ea2",
   "metadata": {},
   "source": [
    "### To create the end point the steps are:\n",
    "\n",
    "1. Create the Model using the Image container and the Model Tarball uploaded earlier\n",
    "2. Create the endpoint config using the following key parameters\n",
    "\n",
    "    a) Instance Type is ml.g5.2xlarge \n",
    "    \n",
    "    b) ContainerStartupHealthCheckTimeoutInSeconds is 15*60 to ensure health check starts after the model is ready\n",
    "    \n",
    "3. Create the end point using the endpoint config created    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd73c8ca-835f-4300-9506-674376102c82",
   "metadata": {
    "tags": []
   },
   "source": [
    "One of the key parameters here is **TENSOR_PARALLEL_DEGREE** which essentially tells the DeepSpeed library to partition the models along 8 GPU's. This is a tunable and configurable parameter.\n",
    "\n",
    "This parameter also controls the no of workers per model which will be started up when DJL serving runs. As an example if we have a 8 GPU machine and we are creating 8 partitions then we will have 1 worker per model to serve the requests. For further reading on DeepSpeed you can follow the link https://www.deepspeed.ai/tutorials/inference-tutorial/#initializing-for-inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08367121-837e-4dc0-9be5-97a3a83adeff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "\n",
    "model_name = name_from_base(f\"baichuan2-7b-base-finetuned\")\n",
    "print(model_name)\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"ModelDataUrl\": s3_code_artifact,\n",
    "    },\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c503b35-787f-4e22-af96-cdfdc254f84e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g5.2xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            #\"VolumeSizeInGB\" : 300,\n",
    "            #\"ModelDataDownloadTimeoutInSeconds\": 15*60,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 15*60,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e824c3-2472-4e1f-8853-061a55e1ade6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d72d7-8b49-4b21-be6c-83c9f29ecc8a",
   "metadata": {},
   "source": [
    "#### Wait for the end point to be created."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24abc448-d60e-4b59-bfc6-65fde9858c25",
   "metadata": {},
   "source": [
    "### This step can take ~ 15 min or longer so please be patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d330f0-4929-4cb2-93cf-2e595c23ca63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bf97ea-474b-46fb-b6c5-3e85b910dc4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84bfaff5-b4a0-4875-95b7-266320e2d30b",
   "metadata": {},
   "source": [
    "#### Leverage the Boto3 to invoke the endpoint. \n",
    "\n",
    "This is a generative model so we pass in a Text as a prompt and Model will complete the sentence and return the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb83cc51-172e-44b8-a00e-e26f669e957c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "parameters = {\n",
    "  \"early_stopping\": True,\n",
    "  \"max_new_tokens\": 128,\n",
    "  # \"min_new_tokens\": 128,\n",
    "  \"do_sample\": True,\n",
    "  # \"temperature\": 1.0,\n",
    "}\n",
    "\n",
    "prompt = \"世界上第二高的山峰是哪座\"\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            Body=json.dumps(\n",
    "            {\n",
    "                \"inputs\"    : prompt,\n",
    "                \"parameters\": parameters\n",
    "            }\n",
    "            ),\n",
    "            ContentType=\"application/json\",\n",
    "        )\n",
    "\n",
    "response_model['Body'].read().decode('utf8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f5d7bd-defe-45fa-985c-1f7cf0958ccb",
   "metadata": {},
   "source": [
    "### Delete the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bb6bdb-6449-4b88-8b62-9c8d069ccd27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153590f9-2659-487a-b002-554c745d7aba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
